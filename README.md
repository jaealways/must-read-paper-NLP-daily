# must-read-paper-daily
List of papers with recent trends for NLP in 2024

## 2022
- [2022 NAACL](https://github.com/jaealways/must-read-paper-daily/blob/main/2022/NAACL.md)

## 2023
- [2023 ACL](https://github.com/jaealways/must-read-paper-daily/blob/main/2023/ACL.md)
- [2023 EACL](https://github.com/jaealways/must-read-paper-daily/blob/main/2023/EACL.md)
- [2023 EMNLP](https://github.com/jaealways/must-read-paper-daily/blob/main/2023/EMNLP.md)
- [2023 NeurIPS](https://github.com/jaealways/must-read-paper-daily/blob/main/2023/NeurIPS.md)
- [2023 WMT](https://github.com/jaealways/must-read-paper-daily/blob/main/2023/WMT.md)


## Updated on 2024.01.08

# Must-read LIST

|Date|Title|Authors|PDF|KOR_SUM|TAGS|TLDR|
|---|---|---|---|---|---|---|
|**01-09**|**A TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS**|Yuqi Nie et.al.|[2211.14730](https://arxiv.org/pdf/2211.14730.pdf)|**[]()**|ICLR'23, time-series| |
|**01-08**|**Do PLMs Know and Understand Ontological Knowledge?**|Weiqi Wu et.al.|[2309.05936](https://arxiv.org/pdf/2309.05936.pdf)|**[]()**|ACL'23, PLM| |
|**01-07**|**QLoRA: Efficient Finetuning of Quantized LLMs**|Tim Dettmers et.al.|[2305.14314](https://arxiv.org/pdf/2305.14314.pdf)|**[]()**|NeurIPS'23, fine-tuning| |
|**01-06**|**Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents**|Weiwei Sun et.al.|[2105.09680](https://arxiv.org/pdf/2105.09680.pdf)|**[]()**|EMNLP'23, LLM| |
|**01-05**|**Marked Personas: Using Natural Language Prompts to Measure Stereotypes in Language Models**|Myra Cheng et.al.|[2305.18189](https://arxiv.org/pdf/2305.18189.pdf)|**[]()**|ACL'23, prompt| |
|**01-04**|**From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models**|Shangbin Feng et.al.|[2305.08283](https://arxiv.org/pdf/2305.08283.pdf)|**[]()**|ACL'23, AI ethics|  |
|**01-03**|**Mamba: Linear-Time Sequence Modeling with Selective State Spaces**|Albert Gu et.al.|[2312.00752](https://arxiv.org/ftp/arxiv/papers/2312/2312.00752.pdf)|**[]()**|ICLR'24, LLM|  |
|**01-02**|**FRUIT: Faithfully Reflecting Updated Information in Text**|Robert L. Logan IV et.al.|[2112.08634](https://arxiv.org/pdf/2112.08634.pdf)|**[]()**|NAACL'22, Information| FRUIT: The generation task to update an article by reflecting new information |
|**01-01**|**Scaling Data-Constrained Language Models**|Niklas Muennighoff et.al.|[2305.16264](https://arxiv.org/pdf/2305.16264.pdf)|**[]()**|NeurIPS'23, LLM| The way to scale the language model in data-constrained regimes |

